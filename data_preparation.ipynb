{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "This notebook takes as input the original dataset and generates a clean dataset. It can run automatically or with some user inputs. User inputs generate a more consistent dataset, but for the dataset used it has no effect on model accuracy. \n",
    "In addition to the automatic mode, the numerical encoding and type of scaler are also parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "auto_mode = True\n",
    "scaler = RobustScaler()\n",
    "numerical_null_values = 'knn'\n",
    "categorical_null_values = 'knn'\n",
    "dataset_name = 'kidney_disease'\n",
    "useless_columns = ['id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if auto_mode not in [True, False]:\n",
    "    raise ValueError('\"auto_mode\" is not of type boolean')\n",
    "if type(scaler) not in [MaxAbsScaler, MinMaxScaler, RobustScaler, StandardScaler]:\n",
    "    raise ValueError('\"scaler\" is not of a scaler class')\n",
    "if numerical_null_values not in ['mode', 'mean', 'knn']:\n",
    "    raise ValueError('\"numerical_null_values\" has to be in [mode, mean, knn]')\n",
    "if categorical_null_values not in ['knn', 'frequent']:\n",
    "    raise ValueError('\"numerical_null_values\" has to be in [frequent, knn]')\n",
    "if not os.path.exists('Data/'+dataset_name+'.csv'):\n",
    "    raise ValueError('No dataset found for name \"'+dataset_name+'\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T00:14:48.381794Z",
     "start_time": "2023-04-28T00:14:48.289172Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_df = pd.read_csv('Data/'+dataset_name+'.csv')\n",
    "original_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.drop(useless_columns, inplace=True, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = original_df.drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Categorical and Numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = original_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = original_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Numerical Columns:\\n\", \"  - \".join(numerical_cols))\n",
    "print(\"\\nCategorical Columns:\\n\", \"  - \".join(categorical_cols))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert false categorical to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified = False\n",
    "for col in categorical_cols:\n",
    "    if any(original_df[col].dropna().apply(lambda x: x.isnumeric() if type(x)==str else False)):\n",
    "        modified = True\n",
    "        print(col+' was categorical')\n",
    "        original_df[col]=pd.to_numeric(original_df[col], errors='coerce')\n",
    "\n",
    "if modified:\n",
    "    numerical_cols = original_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = original_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    print(\"\\nNew numerical Columns:\\n\", \"  - \".join(numerical_cols))\n",
    "    print(\"\\nNew categorical Columns:\\n\", \"  - \".join(categorical_cols))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical columns standardization\n",
    "### Noise removing\n",
    "Some of the categorical values contain misspelled data (example: '\\tyes' for 'yes'). Here, we want to make sure all the misspells get replaced by their correct values.\n",
    "#### User controlled way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T00:14:48.386162Z",
     "start_time": "2023-04-28T00:14:48.324073Z"
    }
   },
   "outputs": [],
   "source": [
    "if not auto_mode:\n",
    "    for column in categorical_cols :\n",
    "        distinct_values =  original_df[column].dropna().unique()\n",
    "        print(distinct_values)\n",
    "        print('\\n' + column + ':')\n",
    "        for value in distinct_values:\n",
    "            is_replaced = input(\"\\t- '\" + value + \"' : replace? (y/N)\")\n",
    "            if is_replaced == 'y':\n",
    "                replacement = input('\\t  replace by: ')\n",
    "                original_df[column] = original_df[column].replace(value, replacement)\n",
    "        distinct_values =  original_df[column].dropna().unique()\n",
    "    \n",
    "    print('\\nBoolean columns now all contain only 2 distinct not-null values')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T00:14:48.386206Z",
     "start_time": "2023-04-28T00:14:48.326921Z"
    }
   },
   "outputs": [],
   "source": [
    "if auto_mode:\n",
    "    original_df[categorical_cols] = original_df[categorical_cols].replace({'\\t':'', ' ':''}, regex=True)\n",
    "    for col in categorical_cols:\n",
    "        original_df[col]=original_df[col].str.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check correction\n",
    "In this dataset, all the categorical columns express a boolean value (examples: [yes, no], [present, notpresent]). We can check that all of the columns don't contain misspels by checking that they all contain only 2 distinct non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_col_check = True\n",
    "for column in categorical_cols :\n",
    "    distinct_values =  original_df[column].dropna().unique()\n",
    "    if len(distinct_values) > 2:\n",
    "        all_col_check = False\n",
    "        print(column+' contains misspells : '+str(distinct_values))\n",
    "\n",
    "if all_col_check:\n",
    "    print('Check is cleared') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String encoding\n",
    "#### User controlled way\n",
    "This way allows to keep some integrity in the data. For example, if yes is encoded as 1 in a column, it will also be encoded as 1 in another column, which we can't make sure of with automatic encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T00:14:55.110513Z",
     "start_time": "2023-04-28T00:14:48.333634Z"
    }
   },
   "outputs": [],
   "source": [
    "if not auto_mode:\n",
    "    boolean_columns = [x for x in categorical_cols if len(original_df[x].dropna().unique())==2]\n",
    "    for column in boolean_columns:\n",
    "        distinct_values =  original_df[column].dropna().unique()\n",
    "        order = True\n",
    "        print(column + ' : ' + str(distinct_values) + ' --> ' + str([order, not order]))\n",
    "        is_reversed = input(\"Confirm order or reverse ? (C/r)\")\n",
    "        if is_reversed == 'r':\n",
    "            order = not order\n",
    "        original_df[column] = original_df[column].replace(distinct_values[0], int(order))\n",
    "        print('\\t- ' + distinct_values[0] + ' --> ' + str(order))\n",
    "        original_df[column] = original_df[column].replace(distinct_values[1], int(not order))\n",
    "        print('\\t- ' + distinct_values[1] + ' --> ' + str(not order) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T00:14:55.110638Z",
     "start_time": "2023-04-28T00:14:55.088507Z"
    }
   },
   "outputs": [],
   "source": [
    "if auto_mode:\n",
    "    for col in categorical_cols:\n",
    "        uniques=original_df[col].dropna().unique()\n",
    "        replace_in_order = [1, 0]\n",
    "        if uniques[0]=='no' or uniques[0].startswith('not') or uniques[0].startswith('ab'):\n",
    "            replace_in_order = [0, 1]\n",
    "        original_df[col] = original_df[col].replace(uniques[0], replace_in_order[0])\n",
    "        original_df[col] = original_df[col].replace(uniques[1], replace_in_order[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.DataFrame(scaler.fit_transform(original_df), columns=original_df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null values handling\n",
    "### Numerical columns\n",
    "We have 3 different ways to replace null values in numerical columns:\n",
    "- Mean\n",
    "- Mode\n",
    "- KNN prediction  \n",
    "\n",
    "As our dataset has an important number of outliers, mode seems like a better choice than mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T00:14:55.110784Z",
     "start_time": "2023-04-28T00:14:55.100237Z"
    }
   },
   "outputs": [],
   "source": [
    "if numerical_null_values in ['mode', 'mean']:\n",
    "    for col in numerical_cols:\n",
    "        if numerical_null_values == 'mode':\n",
    "            replacement = original_df[col].mode()[0]\n",
    "        else:\n",
    "            replacement = original_df[col].mean()\n",
    "        original_df[col] = original_df[col].fillna(replacement)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical columns\n",
    "- most frequent value\n",
    "- KNN prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if categorical_null_values=='frequent':\n",
    "    for col in categorical_cols:\n",
    "        most_frequent = original_df[col].value_counts().idxmax()\n",
    "        original_df[col] = original_df[col].fillna(most_frequent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN prediction\n",
    "KNN prediction is more precise, but consumes more in both cases. As it is done for all the dataset at once, we do it last because otherwise other null values would also be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "if 'knn' in [categorical_null_values, numerical_null_values]:\n",
    "    imputer = KNNImputer(n_neighbors=1)\n",
    "    original_df = pd.DataFrame(imputer.fit_transform(original_df), columns=original_df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Null value check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T00:14:55.118000Z",
     "start_time": "2023-04-28T00:14:55.115587Z"
    }
   },
   "outputs": [],
   "source": [
    "if original_df.isna().values.any():\n",
    "    raise Exception('Dataset still contains null values')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save clean dataset into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.to_csv('Data/'+dataset_name+'-clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
