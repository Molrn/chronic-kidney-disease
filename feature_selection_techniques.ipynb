{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2c452b8",
   "metadata": {},
   "source": [
    "# Feature selection Techniques\n",
    "\n",
    "This notebook consists in detailing the most appropriate features according to different feature selection techniques\n",
    "\n",
    "## Filter Methods \n",
    "\n",
    "Calculate the correlations between the features and target attributes\n",
    "\n",
    "### Divide of features into two categories \n",
    "    - Categorical features (Nominal): albumin - sugar - red_blood_cells  - pus_cell  - pus_cell_clumps  - bacteria  - hypertension  - diabetes_mellitus  - coronary_artery_disease  - appetite  - peda_edema  - aanemia  \n",
    " \n",
    "    - Numearical features (Ordinal) : age  - blood_pressure  - specific_gravity    - blood_glucose_random  - blood_urea  - serum_creatinine  - sodium  - potassium  - hemoglobin  - packed_cell_volume  - white_blood_cell_count  - red_blood_cell_count\n",
    "    \n",
    "\n",
    "To divide them, we checked the values in the dataset of each feature and the meaning behind it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe30ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09437200",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_numfeature_numfeature=0.7\n",
    "correlation_catfeature_catfeature=0.7\n",
    "correlation_numfeature_catfeature=0.7\n",
    "correlation_target_numfeature=0.2\n",
    "correlation_target_catfeature=0.2\n",
    "important_num_features=[]\n",
    "important_cat_features=[]\n",
    "how_to_divide_features_num_cat='default' \n",
    "#  It can also be: automatically / or the user can enter: manually.\n",
    "threshold_unique_values_tobe_cat_feature=5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5289cbe7",
   "metadata": {},
   "source": [
    "## List of personalized functions used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberOfLigne(numberGraphe,numberOfColumn):\n",
    "    return int(math.ceil(numberGraphe/numberOfColumn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768af94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_correlated_feature_feature(mask,list_important_feature):\n",
    "\n",
    "    # will be used for plotting high_corr_pairs and selected_pairs\n",
    "    high_corr_pairs = np.where(mask)\n",
    "    selected_pairs = [(list_important_feature[i], list_important_feature[j]) for i, j in zip(*high_corr_pairs) if i != j]\n",
    "\n",
    "    strongly_correlated_features = corr[mask].stack().dropna().reset_index()\n",
    "    \n",
    "    \n",
    "    return  strongly_correlated_features, selected_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_correlated_feature(strongly_correlated_features,important_features):\n",
    "    deleted_features=[]\n",
    "    for f in important_features:\n",
    "    \n",
    "        if f not in deleted_features:\n",
    "            df=strongly_correlated_features[strongly_correlated_features['level_0']==f]\n",
    "    \n",
    "            mask = np.isin(important_features, df['level_1'].values)\n",
    "               \n",
    "       \n",
    "            \n",
    "            deleted_features=np.concatenate((deleted_features, important_features[mask]), axis=0).copy()\n",
    "            \n",
    "    mask = np.isin(important_features, deleted_features)\n",
    "   \n",
    "    important_features=important_num_features[~mask]\n",
    "    \n",
    "    return important_features,deleted_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc908e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_unique_paris(selected_pairs):\n",
    "    unique_pairs = set()\n",
    "    for pair in selected_pairs:\n",
    "    \n",
    "        sorted_pair = tuple(sorted(pair))\n",
    "        unique_pairs.add(sorted_pair)\n",
    "\n",
    "\n",
    "    unique_pairs_list = list(unique_pairs)\n",
    "    return unique_pairs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_num_cat_manually(columns , target , cat_features,num_features):\n",
    "    for col in columns:\n",
    "        if( col != target):\n",
    "            while True:\n",
    "                    print(f'Enter the type of feature {col} by writing cat or num:')\n",
    "                    type_feature = input()\n",
    "                    if type_feature== 'cat' or type_feature=='num':\n",
    "                        break\n",
    "            if type_feature=='cat':\n",
    "                cat_features.append(col)\n",
    "            else:\n",
    "                num_features.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_num_cat_automatically(columns , target , cat_features,num_features,Threshold):\n",
    "    for col in clean_df:\n",
    "        if( col != target):\n",
    "            if len(clean_df[col].unique()) <= Threshold:\n",
    "                cat_features.append(col)\n",
    "            else:\n",
    "                num_features.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_target(columns):\n",
    "    while True:\n",
    "        print('Donner le nom de la target')\n",
    "        target = input()\n",
    "        if target in columns:\n",
    "            break\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv(r'Data/clean_dataset.csv')\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features=[]\n",
    "num_features=[]\n",
    "if how_to_divide_features_num_cat=='manually':\n",
    "    target=get_name_target(clean_df.columns)\n",
    "    features_num_cat_manually(clean_df.columns , target , cat_features,num_features) \n",
    "elif how_to_divide_features_num_cat=='automatically':\n",
    "    target=get_name_target(clean_df.columns)\n",
    "    features_num_cat_automatically(clean_df.columns , target , cat_features,num_features,threshold_unique_values_tobe_cat_feature)\n",
    "else:\n",
    "    target='classification'\n",
    "    \n",
    "    num_features=['age'  ,\n",
    "              'blood_pressure'  ,\n",
    "              'specific_gravity'    ,\n",
    "              'blood_glucose_random'  ,\n",
    "              'blood_urea'  ,\n",
    "              'serum_creatinine'  ,\n",
    "              'sodium'  ,\n",
    "              'potassium'  ,\n",
    "              'hemoglobin'  ,\n",
    "              'packed_cell_volume',\n",
    "              'white_blood_cell_count'  ,\n",
    "              'red_blood_cell_count']\n",
    "    \n",
    "    cat_features=['albumin' ,\n",
    "              'sugar' ,\n",
    "              'red_blood_cells'  ,\n",
    "              'pus_cell'  ,\n",
    "              'pus_cell_clumps'  ,\n",
    "              'bacteria'  ,\n",
    "              'hypertension'   ,\n",
    "              'diabetes_mellitus'  ,\n",
    "              'coronary_artery_disease'  ,\n",
    "              'appetite'   ,\n",
    "              'peda_edema'  ,\n",
    "              'aanemia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6463687",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a86b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = np.concatenate((cat_features, num_features), axis=0)\n",
    "all_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56f85f7f",
   "metadata": {},
   "source": [
    "### Variance test\n",
    "The objective of this test is to determine the constants in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold = 0)\n",
    "selector.fit(clean_df)\n",
    "\n",
    "print(selector.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e96bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the name of columns that are constant value false\n",
    "columns_to_delete =clean_df.columns[np.logical_not(selector.get_support())]\n",
    "\n",
    "print(columns_to_delete)\n",
    "\n",
    "clean_df = clean_df.drop(columns=columns_to_delete)\n",
    "\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('conclusion')\n",
    "if len(columns_to_delete) ==0:\n",
    "    print('There is no constant in the project')\n",
    "else :\n",
    "    print('columns deleted : ',columns_to_delete)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf546aca",
   "metadata": {},
   "source": [
    "### Correlation between numerical features and  the target (categorical variable) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=[]\n",
    "for feature in num_features:\n",
    "    pb_corr, pb_p_value = stats.pointbiserialr(   # Used between a binary categorical variable and numerical one\n",
    "        clean_df[feature],\n",
    "        clean_df[target])\n",
    "    corr.append([feature,abs(pb_corr)])\n",
    "corr=pd.DataFrame(corr,columns=['num_feature','correlation with the target'])\n",
    "\n",
    "display(corr.sort_values(by='correlation with the target'  ,ascending=False))\n",
    "\n",
    "features=corr[corr['correlation with the target']>correlation_target_numfeature].sort_values(by='correlation with the target'  ,ascending=False)\n",
    "\n",
    "\n",
    "important_num_features=  features['num_feature'].values.copy() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbab7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('conclusion :')\n",
    "print(f'Important features according to the parametres correlation_target_feature {correlation_target_numfeature}')\n",
    "print('-----------------------------')\n",
    "print(important_num_features)\n",
    "print('-----------------------------')\n",
    "print(f'features deleted according to the parametres correlation_target_feature {correlation_target_numfeature}')\n",
    "print(corr[corr['correlation with the target']<correlation_target_numfeature])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765503ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resultat by ploting some graphs\n",
    "# for the different 3 features (hemoglobin , packed_cell_volume ,specific_gravity) high correlated with the target\n",
    "# for the different 3 features (white_blood_cell_count , potassium , age) that are low correlated with the target \n",
    "\n",
    "plt.figure(figsize = (20, 25))\n",
    "plotnumber=1\n",
    "\n",
    "numberGrapheColumn=2\n",
    "numberGrapheLigne =numberOfLigne(len(important_num_features),numberGrapheColumn)\n",
    "\n",
    "for feature in important_num_features:\n",
    "    ax = plt.subplot(numberGrapheLigne, numberGrapheColumn, plotnumber)\n",
    "    sns.boxplot(x=target, y=feature, data=clean_df)\n",
    "    plotnumber+=1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Can confirm the resultats by the graphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "785974af",
   "metadata": {},
   "source": [
    "### Correlation between numerical features (Pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7add8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=clean_df[important_num_features].corr()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(corr, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c6dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (abs(corr) > correlation_numfeature_numfeature) & (abs(corr) <1)\n",
    "\n",
    "strongly_correlated_features,selected_pairs=detect_correlated_feature_feature(mask,important_num_features)\n",
    "\n",
    "\n",
    "display(strongly_correlated_features)\n",
    "\n",
    "print(selected_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a490a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(selected_pairs)>0:\n",
    "    important_num_features,deleted_features=delete_correlated_feature(strongly_correlated_features,important_num_features)\n",
    "    print(deleted_features)\n",
    "    print(important_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f88f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Conclusion')\n",
    "if len(selected_pairs)>0:\n",
    "    print('------------------------')\n",
    "    print(f\"feature deleted due to high correlation \",deleted_features)\n",
    "    print('------------------------')\n",
    "    print(f\"numerical feature:\",important_num_features)\n",
    "else:\n",
    "    print('There is no correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76064dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names for the high correlation pairs\n",
    "\n",
    "selected_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bad56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs_list=transform_unique_paris(selected_pairs)\n",
    "\n",
    "print(unique_pairs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ca1b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the resultat by ploting some graph\n",
    "# for the different features (hemoglobin , packed_cell_volume ,red_blood_cell_count) are correlated\n",
    "# (hemoglobin , potassium , age) that are not correlated \n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "plotnumber=1\n",
    "\n",
    "\n",
    "numberGrapheLigne =numberOfLigne(len(unique_pairs_list),numberGrapheColumn)\n",
    "\n",
    "for features in unique_pairs_list:\n",
    "    ax = plt.subplot(numberGrapheLigne, numberGrapheColumn, plotnumber)\n",
    "    sns.scatterplot(data=clean_df, x=features[0],y=features[1])\n",
    "    plotnumber+=1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Can confirm the resultats by the graphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f746d21",
   "metadata": {},
   "source": [
    "### Correlation between categorical features and the categorical target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac89713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carmerV(clean_df,col1,col2):\n",
    "    contingency_table = pd.crosstab(clean_df[col1], clean_df[col2])\n",
    "    \n",
    "\n",
    "    # Calculate CramÃ©r's V\n",
    "    chi2 = stats.chi2_contingency(contingency_table)[0]\n",
    "    n = contingency_table.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = contingency_table.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    rcorr = r - ((r - 1)**2) / (n - 1)\n",
    "    kcorr = k - ((k - 1)**2) / (n - 1)\n",
    "    V = np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=[]\n",
    "for col in cat_features:\n",
    "    V=carmerV(clean_df,col,target)\n",
    "    corr.append([col,V])\n",
    "    \n",
    "\n",
    "corr=pd.DataFrame(corr,columns=['cat_feature','correlation with the target'])\n",
    "\n",
    "corr.sort_values(by='correlation with the target'  ,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=corr[corr['correlation with the target']>correlation_target_catfeature].sort_values(by='correlation with the target'  ,ascending=False)\n",
    "\n",
    "\n",
    "important_cat_features=  features['cat_feature'].values.copy() \n",
    "\n",
    "important_cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61108725",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('conclusion :')\n",
    "print(f'Important features according to the parametres correlation_target_feature {correlation_target_catfeature}')\n",
    "print('-----------------------------')\n",
    "print(important_cat_features)\n",
    "print('-----------------------------')\n",
    "print(f'features deleted according to the parametres correlation_target_feature {correlation_target_catfeature}')\n",
    "print(corr[corr['correlation with the target']<correlation_target_catfeature])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff626b79",
   "metadata": {},
   "source": [
    "### Correlation between categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7448b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows= []\n",
    "\n",
    "for f1 in cat_features:\n",
    "    col = []\n",
    "    for f2 in cat_features:\n",
    "        v=carmerV(clean_df,f1,f2)\n",
    "        col.append(v)\n",
    "    rows.append(col)\n",
    "    \n",
    "cramers_results = np.array(rows)\n",
    "corr_cat_features = pd.DataFrame(cramers_results, columns = cat_features, index =cat_features)\n",
    "corr_cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (corr_cat_features > correlation_catfeature_catfeature) & (corr_cat_features <0.97)\n",
    "mask\n",
    "\n",
    "strongly_correlated_features, selected_pairs=detect_correlated_feature_feature(mask,important_cat_features)\n",
    "\n",
    "display(strongly_correlated_features)\n",
    "\n",
    "print(selected_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eaed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(selected_pairs)>0:\n",
    "    important_cat_features,deleted_features=delete_correlated_feature(strongly_correlated_features,important_cat_features)\n",
    "    \n",
    "    print(important_cat_features)\n",
    "    \n",
    "    print(deleted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Conclusion')\n",
    "if len(selected_pairs)>0:\n",
    "    print('------------------------')\n",
    "    print(f\"feature deleted due to high correlation \",deleted_features)\n",
    "    print('------------------------')\n",
    "    print(f\"numerical feature:\",important_cat_features)\n",
    "else:\n",
    "    print('There is no correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(selected_pairs)>0:\n",
    "    \n",
    "    unique_pairs_list=transform_unique_paris(selected_pairs)\n",
    "\n",
    "    print(unique_pairs_list)\n",
    "\n",
    "    plt.figure(figsize = (15, 7))\n",
    "    plotnumber=1\n",
    "\n",
    "\n",
    "    numberGrapheLigne =numberOfLigne(len(unique_pairs_list),numberGrapheColumn)\n",
    "\n",
    "    for features in unique_pairs_list:\n",
    "        ax = plt.subplot(numberGrapheLigne, numberGrapheColumn, plotnumber)\n",
    "        sns.scatterplot(data=clean_df, x=features[0],y=features[1])\n",
    "        plotnumber+=1\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50fd8b2e",
   "metadata": {},
   "source": [
    "### Correlation between categorical features and num features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7b5c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,200))\n",
    "plotnumber=1\n",
    "\n",
    "\n",
    "for cat_feature in  important_cat_features:\n",
    "    f_scores, p_values = f_classif(clean_df[important_num_features],clean_df[cat_feature])\n",
    "    \n",
    "    # Print the results for the current target variable\n",
    "    for j, feature in enumerate(important_num_features):\n",
    "        if p_values[j]>0.05 :\n",
    "            print(f\" {feature} and {cat_feature}: F-score = {f_scores[j]}, p-value = {p_values[j]}\")\n",
    "            ax = plt.subplot(len(important_cat_features)*len(important_num_features), 1, plotnumber)\n",
    "            sns.boxplot(x=cat_feature, y=feature, data=clean_df)\n",
    "            plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1acfe846",
   "metadata": {},
   "source": [
    "### Store the important features into json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299fb206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the list of feature into an excel file \n",
    "import json\n",
    "\n",
    "important_features = np.concatenate((important_num_features, important_cat_features), axis=0).tolist()\n",
    "print(important_features)\n",
    "# Open the file in write mode\n",
    "with open('important_features.json', 'w') as file:\n",
    "    # Write the array to the file in JSON format\n",
    "    json.dump(important_features, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b022e22",
   "metadata": {},
   "source": [
    "## Feature importance using tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "                                  random_state=0)\n",
    "\n",
    "rf.fit(clean_df[all_features], clean_df[target])\n",
    "\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c1c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = feature_importances.argsort()[::-1]\n",
    "sorted_feature_importances = feature_importances[indices]\n",
    "sorted_feature_names = [all_features[i] for i in indices]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(len(sorted_feature_importances)), sorted_feature_importances)\n",
    "plt.xticks(range(len(sorted_feature_importances)), sorted_feature_names, rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de80f639",
   "metadata": {},
   "source": [
    "## Feature importance using Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(clean_df[all_features], clean_df[target], random_state=1)\n",
    "my_model = RandomForestClassifier(n_estimators=100,\n",
    "                                  random_state=0).fit(train_X, train_y)\n",
    "\n",
    "\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n",
    "eli5.show_weights(perm, feature_names = val_X.columns.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b77eaadd",
   "metadata": {},
   "source": [
    "## Feature importance using  Coefficients in  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319bb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(clean_df[all_features], clean_df[target])\n",
    "\n",
    "coefficients = logreg.coef_[0]\n",
    "\n",
    "# Get the absolute feature importance\n",
    "feature_importances = np.abs(coefficients)\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(len(sorted_idx)), feature_importances[sorted_idx])\n",
    "plt.xticks(range(len(sorted_idx)), [clean_df[all_features].columns[i] for i in sorted_idx], rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Feature Importances (Logistic Regression)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70828255",
   "metadata": {},
   "source": [
    "## Represent the data in other format and check if it improve the resultat of the machine learning algorthms "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39bbf03a",
   "metadata": {},
   "source": [
    "### PCA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca= PCA (n_components=0.99,whiten=True)\n",
    "\n",
    "pca_data=pca.fit_transform(clean_df)\n",
    "variance_explained =pca.explained_variance_ratio_\n",
    "print('The variance added by each component')\n",
    "print(variance_explained)\n",
    "print('number of PCA is ',len(variance_explained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_variance_ratio = np.cumsum(variance_explained)\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Variance Explained by Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9d1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1], hue=clean_df[target], palette='viridis')\n",
    "\n",
    "# Manually set the legend labels\n",
    "scatter.legend_.set_title('classification')\n",
    "scatter.legend_.texts = ['ckd', 'notckd']\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Scatter Plot with 2 PC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3fc1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# axes instance\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "\n",
    "# get colormap from seaborn\n",
    "cmap = ListedColormap(sns.color_palette(\"husl\", 2).as_hex())\n",
    "\n",
    "# plot\n",
    "sc = ax.scatter(pca_data[:, 0], pca_data[:, 1], pca_data[:, 2], s=40, c=clean_df[target], marker='o', cmap=cmap, alpha=1)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('Scatter Plot with 3 PC')\n",
    "\n",
    "\n",
    "# legend\n",
    "plt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_columns_pc = [f'PC{i}' for i in range(1, len(variance_explained) + 1)]\n",
    "name_columns_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(data=pca_data,columns=name_columns_pc)\n",
    "df_pca=pd.concat((df_pca, clean_df[target]), axis=1)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved it to excel file \n",
    "df_pca.to_csv('Data/pca_dataset.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "155a23d4",
   "metadata": {},
   "source": [
    "### LDA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c1f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Fit the LDA model on the training data\n",
    "X_lda=lda.fit_transform(clean_df[all_features], clean_df[target])\n",
    "\n",
    "X_lda.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a298f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_lda, clean_df[target], c=clean_df[target], cmap='viridis')\n",
    "plt.xlabel('LDA Component')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Scatter Plot of LDA-Transformed Data')\n",
    "plt.colorbar(label='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_columns_lda = [f'LDA{i}' for i in range(1, len(lda.explained_variance_ratio_) + 1)]\n",
    "name_columns_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c13bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda = pd.DataFrame(data=X_lda,columns=name_columns_lda)\n",
    "df_lda=pd.concat((df_lda, clean_df[target]), axis=1)\n",
    "df_lda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved it to excel file \n",
    "df_lda.to_csv('Data/lda_dataset.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "743b8be4",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "- Automation of feature importance for correlation / and feature importance (reorganise et creation d'un fichier json qui contient les valuers)\n",
    "- Automation of dataset division selon la variation c'est esay \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
