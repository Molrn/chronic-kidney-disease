{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'kidney_disease'\n",
    "target_column_name = 'classification' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4518d8c5",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "This step consists in finding the best parameters to set each model.\n",
    "## Select Data and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clean_df = pd.read_csv('Data/'+dataset_name+'/clean_dataset.csv')\n",
    "\n",
    "all = clean_df.columns.drop(target_column_name)\n",
    "data_df = clean_df[all]\n",
    "target_df = clean_df[target_column_name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1593674b",
   "metadata": {},
   "source": [
    "## Tuner\n",
    "Several pre-made solutions already exist to tune these parameters: random search, grid search, halving search, etc... Instead of using one of these, we decided to build our own solution. This solution will find a local optimum without much computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "def localOptimumSearchCV(model_class, param_grid, X, y, cv_scoring=True, cv=5, test_size=0.2, n_iter_max=100, depth_rate=0.25, max_time=120):\n",
    "    \"\"\"Find a local optimum set of parameters for model class  \n",
    "    To do so, an initial set is improved iteratively by increasing progressively each parameter.\n",
    "    If the parameter change creates a score improvement, it becomes the set to explore \n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "    model_class : class of the model to tune\n",
    "        provides a ``fit`` and a ``score`` function.\n",
    "\n",
    "    param_grid : dict\n",
    "        Dictionary with parameters names (string) as keys and lists of\n",
    "        parameter settings to try as values\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where `n_samples` is the number of samples and\n",
    "        `n_features` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples, n_output) or (n_samples,)\n",
    "        Target relative to X for classification or regression.\n",
    "\n",
    "    cv_scoring : boolean, default=True\n",
    "        Use cross validation for scoring. If false, train-test scoring is used\n",
    "    \n",
    "    cv : int, default=5\n",
    "        Number of folds to split the dataset into in order to execute cross-validation\n",
    "    \n",
    "    test_size : float, default=0.2\n",
    "        define the \n",
    "\n",
    "    n_iter_max : int, default=100\n",
    "        Number of parameter settings that are tried. If local optimum is found earlier, search is stopped.\n",
    "    \n",
    "    depth_rate : int, default=0.25\n",
    "        Rate from which the depth computed on every parameter before stopping is computed. \n",
    "        Each parameter is computed to a depth of max(1,int(depth_rate*nb_parameter_values)).\n",
    "\n",
    "    max_time : int, default=120\n",
    "        max execution time (seconds)\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    (dict, float) : dictionnary of best parameter set found and score of the model with this set\n",
    "    \"\"\"\n",
    "    params = set(param_grid.keys())\n",
    "    tried_params = set()\n",
    "    n_iter = 0\n",
    "    best_score = 0\n",
    "    empty_params_dict = {}\n",
    "    max_depth = {}\n",
    "    computed_param_sets = []\n",
    "    best_param_indexes = {}\n",
    "    max_sets = 1\n",
    "    for param in params:\n",
    "        max_sets *= len(param_grid[param])\n",
    "        empty_params_dict[param]=0\n",
    "        max_depth[param] = max(1,int(depth_rate*len(param_grid[param])))\n",
    "        best_param_indexes[param] = random.randint(0, len(param_grid[param])-1) \n",
    "\n",
    "    if max_sets < n_iter_max:\n",
    "        for param in params:\n",
    "            max_depth[param]=len(param_grid[param])\n",
    "    current_depth = empty_params_dict.copy()\n",
    "\n",
    "    def indexes_to_params(param_indexes:dict):\n",
    "        param_dict = {}\n",
    "        for param in params:\n",
    "            param_dict[param] = param_grid[param][param_indexes[param]]\n",
    "        return param_dict\n",
    "\n",
    "    start_time = time.time()    \n",
    "\n",
    "    if not cv_scoring :\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    while n_iter < n_iter_max and tried_params != params and best_score != 1 and time.time()-start_time < max_time:\n",
    "        n_iter += 1\n",
    "        param = random.choice(tuple(params.difference(tried_params)))\n",
    "        current_depth[param] += 1\n",
    "        if current_depth[param] >= max_depth[param]:\n",
    "            tried_params.add(param)\n",
    "\n",
    "        param_indexes = best_param_indexes.copy()\n",
    "        param_indexes[param] = (param_indexes[param]+current_depth[param])%len(param_grid[param])\n",
    "        if param_indexes not in computed_param_sets:\n",
    "            computed_param_sets.append(param_indexes)\n",
    "            test_params = indexes_to_params(param_indexes) \n",
    "\n",
    "            model = model_class(**test_params)\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    if cv_scoring:\n",
    "                        score = mean(cross_val_score(model, X, y, cv=cv))\n",
    "                    else:\n",
    "                        model.fit(x_train, y_train)\n",
    "                        score = model.score(x_test, y_test)\n",
    "                if score >= best_score:\n",
    "                    best_score = score\n",
    "                    best_param_indexes = param_indexes\n",
    "                    current_depth = empty_params_dict.copy()\n",
    "                    tried_params = set()\n",
    "            except:\n",
    "                max_depth[param] = len(param_grid[param])\n",
    "                \n",
    "    return indexes_to_params(best_param_indexes), best_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4ddc467",
   "metadata": {},
   "source": [
    "## Model tuning application\n",
    "### Model imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83d8bfab",
   "metadata": {},
   "source": [
    "### Parameter grids definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12812c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_parameters = []\n",
    "model_parameters.append({\n",
    "    'model_class' : KNeighborsClassifier,\n",
    "    'name': 'KNN',\n",
    "    'param_grid' : { \n",
    "        'n_neighbors' : range(2,20),\n",
    "        'weights' : ['uniform','distance'],\n",
    "        'metric' : ['minkowski','euclidean','manhattan']\n",
    "    }\n",
    "})\n",
    "model_parameters.append({\n",
    "    'model_class' : RandomForestClassifier,\n",
    "    'name' : 'Random Forest',\n",
    "    'param_grid' : {\n",
    "        'n_estimators': [5, 10, 50, 100, 500, 1000],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': list(range(5, 55, 5))+[None],\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'min_samples_split': range(2, 10),\n",
    "        'min_impurity_decrease': np.arange(0, 0.1, 0.01),\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    \n",
    "})\n",
    "\n",
    "model_parameters.append({\n",
    "    'model_class' : DecisionTreeClassifier,\n",
    "    'name': 'Decision Tree',\n",
    "    'param_grid' : {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': list(range(5, 55))+[None],\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'min_samples_split': range(2, 10),\n",
    "        'min_impurity_decrease': np.arange(0, 0.1, 0.01)\n",
    "    }\n",
    "})\n",
    "\n",
    "model_parameters.append({\n",
    "    'model_class': LogisticRegression,\n",
    "    'name': 'Logistic Regression',\n",
    "    'param_grid': {\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "        'C': [100, 10, 1.0, 0.1, 0.01],\n",
    "        'max_iter': [1000]\n",
    "    }\n",
    "})\n",
    "\n",
    "model_parameters.append({\n",
    "    'model_class': GaussianNB,\n",
    "    'name': 'Naive Bayes',\n",
    "    'param_grid': {'var_smoothing': [1e-12, 1e-11, 1e-10, 1e-9, 1e-8]}\n",
    "})\n",
    "\n",
    "model_parameters.append({\n",
    "    'model_class': LinearDiscriminantAnalysis,\n",
    "    'name': 'LDA',\n",
    "    'param_grid': {\n",
    "        'solver' : ['svd', 'lsqr', 'eigen'],\n",
    "        'shrinkage': [None, 'auto', 0, 0.5, 1]\n",
    "    }\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "258a29f7",
   "metadata": {},
   "source": [
    "### Optimized parameters computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "optimized_parameters = []\n",
    "for model in model_parameters:\n",
    "    best_params, best_score = localOptimumSearchCV(model['model_class'], model['param_grid'], data_df, target_df,cv_scoring=False, n_iter_max=500, cv=5)\n",
    "        \n",
    "    print(model['name'].ljust(25), best_score)\n",
    "    optimized_parameters.append({\n",
    "        'model_class_name'  : model['model_class'].__name__,\n",
    "        'best_params'       : best_params,\n",
    "        'best_score'        : best_score,\n",
    "        'name'              : model['name']\n",
    "    })\n",
    "\n",
    "file = open('Data/'+dataset_name+'/tuned_hyperparameters.json', 'w')\n",
    "json.dump(optimized_parameters, file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
